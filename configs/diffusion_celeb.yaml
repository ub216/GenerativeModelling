experiment:
  name: "Generative"

model:
  type: "diffusion"                     # options: vae, diffusion, flow, etc.
  params:
    timesteps: 1000
    test_timesteps: 1000
    channel_mults: [1, 2, 4, 8]
    num_blocks: [1, 2, 2, 2]
    use_attention: true
    time_emb_dim: 128
    text_emb_dim: null                   # Set to null if CFG not required
    sample_condition_weight: 10
    renormalise: true
  checkpoint: null                     # Use for evaluation or restarting training

optimizer:                              
  type: "adamw"                          
  params:
    lr: 0.0001
    weight_decay: 0.01
  accumulate_steps: 1                   # Number of steps to accumulate gradients over
  max_grad_norm: 1.0                    # Set to inf to disable gradient clipping  

training:
  batch_size: 192
  epochs: 300
  device: "cuda"
  metric_interval: 300
  ema_decay: 0.9999                    # Set to 0 to disable EMA
  save_after_epoch: 100                 # Save all checkpoints after this epoch

evaluation:
  samples: 100                          # Only used for evaluation run
    
dataset:
  type: "celeb"                         # options: mnist (imagenet, coco, .. in future)
  params:
    root: "/data/datasets/CelebA"
    num_workers: 8
    pin_memory: true
    persistent_workers: true
    image_size: [64, 64]

loss:
  type: "pair_smooth"                      # options: vae_loss, pair_mse
  params:
    reduction: "mean"

metrics:
  fid:
    type: "fid_inception"
    params: 
      samples: 8000